{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SA-2.ipynb","provenance":[],"authorship_tag":"ABX9TyM+T0Vo//0vKTBl8EWY48dM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BYqU4xnNyUD3","colab_type":"text"},"source":["# 2 - Updated Sentiment Analysis\n","\n","We will use:\n","\n","packed padded sequences,\n","pre-trained word embeddings,\n","different RNN architecture,\n","bidirectional RNN,\n","multi-layer RNN,\n","regularization,\n","a different optimizer\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jiKL-c08yP0v","colab_type":"text"},"source":["Source: https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb"]},{"cell_type":"code","metadata":{"id":"KODHrqQKuzT4","colab_type":"code","colab":{}},"source":["import torch\n","from torchtext import data\n","from torchtext import datasets\n","\n","SEED = 1234\n","\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n","LABEL = data.LabelField(dtype = torch.float)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F71WoJ-6u8NY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"18a04109-95d2-47de-9b3f-6afb46ddd9b7","executionInfo":{"status":"ok","timestamp":1586717911078,"user_tz":-330,"elapsed":132465,"user":{"displayName":"Raman Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizDiAb3QZfrxaXJERGc5nC0P5dLr9DgXSr8NJHog=s64","userId":"17108701977930542531"}}},"source":["from torchtext import datasets\n","\n","train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["downloading aclImdb_v1.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:08<00:00, 10.2MB/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Cglc9AAtvBYI","colab_type":"code","colab":{}},"source":["\n","import random\n","\n","train_data, valid_data = train_data.split(random_state = random.seed(SEED))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_DYcx6hPvN4X","colab_type":"text"},"source":["Use of pretrained Glove Vector"]},{"cell_type":"code","metadata":{"id":"msEaBAQUvL3X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"7a765317-6cc5-4272-ea90-7b4d5b468346","executionInfo":{"status":"ok","timestamp":1586718357746,"user_tz":-330,"elapsed":577100,"user":{"displayName":"Raman Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizDiAb3QZfrxaXJERGc5nC0P5dLr9DgXSr8NJHog=s64","userId":"17108701977930542531"}}},"source":["MAX_VOCAB_SIZE = 25_000\n","\n","TEXT.build_vocab(train_data, \n","                 max_size = MAX_VOCAB_SIZE, \n","                 vectors = \"glove.6B.100d\", \n","                 unk_init = torch.Tensor.normal_)\n","\n","LABEL.build_vocab(train_data)"],"execution_count":5,"outputs":[{"output_type":"stream","text":[".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                           \n","100%|█████████▉| 399130/400000 [00:23<00:00, 17348.28it/s]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"AluwZCzRvU5H","colab_type":"code","colab":{}},"source":["BATCH_SIZE = 64\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE,\n","    sort_within_batch = True,\n","    device = device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0dZLlHLdvzgJ","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","\n","class RNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n","                 bidirectional, dropout, pad_idx):\n","        \n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","        \n","        self.rnn = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           bidirectional=bidirectional, \n","                           dropout=dropout)\n","        \n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, text, text_lengths):\n","        \n","        #text = [sent len, batch size]\n","        \n","        embedded = self.dropout(self.embedding(text))\n","        \n","        #embedded = [sent len, batch size, emb dim]\n","        \n","        #pack sequence\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n","        \n","        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n","        \n","        #unpack sequence\n","        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n","\n","        #output = [sent len, batch size, hid dim * num directions]\n","        #output over padding tokens are zero tensors\n","        \n","        #hidden = [num layers * num directions, batch size, hid dim]\n","        #cell = [num layers * num directions, batch size, hid dim]\n","        \n","        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n","        #and apply dropout\n","        \n","        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","                \n","        #hidden = [batch size, hid dim * num directions]\n","            \n","        return self.fc(hidden)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIZAnWTSwBdC","colab_type":"code","colab":{}},"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","N_LAYERS = 2\n","BIDIRECTIONAL = True\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = RNN(INPUT_DIM, \n","            EMBEDDING_DIM, \n","            HIDDEN_DIM, \n","            OUTPUT_DIM, \n","            N_LAYERS, \n","            BIDIRECTIONAL, \n","            DROPOUT, \n","            PAD_IDX)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylX72RVZwH2Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9cfe26d6-8668-48e9-e43b-39cc269d5fc4","executionInfo":{"status":"ok","timestamp":1586718359470,"user_tz":-330,"elapsed":1701,"user":{"displayName":"Raman Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizDiAb3QZfrxaXJERGc5nC0P5dLr9DgXSr8NJHog=s64","userId":"17108701977930542531"}}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["The model has 4,810,857 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"56ZTdyAswij-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"429549af-ba72-402f-8d5c-dd9a6db303c3","executionInfo":{"status":"ok","timestamp":1586718359472,"user_tz":-330,"elapsed":1691,"user":{"displayName":"Raman Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizDiAb3QZfrxaXJERGc5nC0P5dLr9DgXSr8NJHog=s64","userId":"17108701977930542531"}}},"source":["pretrained_embeddings = TEXT.vocab.vectors\n","\n","print(pretrained_embeddings.shape)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["torch.Size([25002, 100])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v2pL-sf7wv-q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":134},"outputId":"04141e49-d154-4979-9a49-528ebf6d8706","executionInfo":{"status":"ok","timestamp":1586718359474,"user_tz":-330,"elapsed":1688,"user":{"displayName":"Raman Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizDiAb3QZfrxaXJERGc5nC0P5dLr9DgXSr8NJHog=s64","userId":"17108701977930542531"}}},"source":["model.embedding.weight.data.copy_(pretrained_embeddings)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n","        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [ 0.2334,  0.1443,  0.0915,  ...,  0.4786,  0.0872,  0.5014],\n","        [ 0.1547,  0.7479,  0.6203,  ...,  0.1398,  0.2157, -0.3645],\n","        [-0.8719,  0.4549, -0.0166,  ..., -0.3637, -0.2198,  0.5020]])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"2xKeETqew5Pj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":134},"outputId":"65cd07f6-9131-41ca-f7bc-ff1ea4859799","executionInfo":{"status":"ok","timestamp":1586718359476,"user_tz":-330,"elapsed":1687,"user":{"displayName":"Raman Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizDiAb3QZfrxaXJERGc5nC0P5dLr9DgXSr8NJHog=s64","userId":"17108701977930542531"}}},"source":["\n","UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","\n","model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n","\n","print(model.embedding.weight.data)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [ 0.2334,  0.1443,  0.0915,  ...,  0.4786,  0.0872,  0.5014],\n","        [ 0.1547,  0.7479,  0.6203,  ...,  0.1398,  0.2157, -0.3645],\n","        [-0.8719,  0.4549, -0.0166,  ..., -0.3637, -0.2198,  0.5020]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r1pHHjg1xLF8","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dxF3M1ENxejH","colab_type":"code","colab":{}},"source":["criterion = nn.BCEWithLogitsLoss()\n","\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"65ZB8wx4xgzq","colab_type":"code","colab":{}},"source":["def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nr65JxRrxi7P","colab_type":"code","colab":{}},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch in iterator:\n","        \n","        optimizer.zero_grad()\n","        \n","        text, text_lengths = batch.text\n","        \n","        predictions = model(text, text_lengths).squeeze(1)\n","        \n","        loss = criterion(predictions, batch.label)\n","        \n","        acc = binary_accuracy(predictions, batch.label)\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KzY1HS6yxrwZ","colab_type":"code","colab":{}},"source":["def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","\n","            text, text_lengths = batch.text\n","            \n","            predictions = model(text, text_lengths).squeeze(1)\n","            \n","            loss = criterion(predictions, batch.label)\n","            \n","            acc = binary_accuracy(predictions, batch.label)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xcmoV7Urxus9","colab_type":"code","colab":{}},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKXYiGIyxx_Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"2fd1329c-1032-470b-c0e1-2bb0cbeae4e7","executionInfo":{"status":"ok","timestamp":1586718576571,"user_tz":-330,"elapsed":218753,"user":{"displayName":"Raman Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizDiAb3QZfrxaXJERGc5nC0P5dLr9DgXSr8NJHog=s64","userId":"17108701977930542531"}}},"source":["N_EPOCHS = 5\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut2-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["\r100%|█████████▉| 399130/400000 [00:40<00:00, 17348.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 41s\n","\tTrain Loss: 0.678 | Train Acc: 57.54%\n","\t Val. Loss: 0.662 |  Val. Acc: 60.09%\n","Epoch: 02 | Epoch Time: 0m 41s\n","\tTrain Loss: 0.610 | Train Acc: 66.71%\n","\t Val. Loss: 0.458 |  Val. Acc: 79.94%\n","Epoch: 03 | Epoch Time: 0m 41s\n","\tTrain Loss: 0.454 | Train Acc: 79.53%\n","\t Val. Loss: 0.387 |  Val. Acc: 83.90%\n","Epoch: 04 | Epoch Time: 0m 41s\n","\tTrain Loss: 0.372 | Train Acc: 84.25%\n","\t Val. Loss: 0.333 |  Val. Acc: 85.56%\n","Epoch: 05 | Epoch Time: 0m 41s\n","\tTrain Loss: 0.319 | Train Acc: 86.95%\n","\t Val. Loss: 0.324 |  Val. Acc: 86.44%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-n8Hl_H0x3dm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"137d5e31-7c51-452c-a0bd-94d680dfdf61","executionInfo":{"status":"ok","timestamp":1586718590523,"user_tz":-330,"elapsed":232702,"user":{"displayName":"Raman Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizDiAb3QZfrxaXJERGc5nC0P5dLr9DgXSr8NJHog=s64","userId":"17108701977930542531"}}},"source":["model.load_state_dict(torch.load('tut2-model.pt'))\n","\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Test Loss: 0.327 | Test Acc: 86.18%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3uqmxWLUx6DO","colab_type":"code","colab":{}},"source":["\n","import spacy\n","nlp = spacy.load('en')\n","\n","def predict_sentiment(model, sentence):\n","    model.eval()\n","    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n","    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n","    length = [len(indexed)]\n","    tensor = torch.LongTensor(indexed).to(device)\n","    tensor = tensor.unsqueeze(1)\n","    length_tensor = torch.LongTensor(length)\n","    prediction = torch.sigmoid(model(tensor, length_tensor))\n","    return prediction.item()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8m-PCo4yGHg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9fa4aa43-47a5-4aaa-ac90-47e1d44a8589","executionInfo":{"status":"ok","timestamp":1586718591817,"user_tz":-330,"elapsed":225100,"user":{"displayName":"Raman Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizDiAb3QZfrxaXJERGc5nC0P5dLr9DgXSr8NJHog=s64","userId":"17108701977930542531"}}},"source":["predict_sentiment(model, \"This film is terrible\")"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.012171247974038124"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"-tKmi746yIJ_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f046c27b-4383-4680-cc04-29d7f3c264cc","executionInfo":{"status":"ok","timestamp":1586718591818,"user_tz":-330,"elapsed":224326,"user":{"displayName":"Raman Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizDiAb3QZfrxaXJERGc5nC0P5dLr9DgXSr8NJHog=s64","userId":"17108701977930542531"}}},"source":["predict_sentiment(model, \"This film is great\")"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9925940632820129"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"Q4AYIJttyN6U","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}