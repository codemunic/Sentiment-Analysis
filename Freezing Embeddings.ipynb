{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Freezing Embeddings.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP6W2OehAwWcP5KJpsapwT9"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cqZpBAcfUsQ-","colab_type":"text"},"source":["# Freezing and Unfreezing Embeddings"]},{"cell_type":"code","metadata":{"id":"_npsg2gXUmtj","colab_type":"code","colab":{}},"source":["N_EPOCHS = 10\n","FREEZE_FOR = 5\n","\n","best_valid_loss = float('inf')\n","\n","#freeze embeddings\n","model.embedding.weight.requires_grad = unfrozen = False\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s | Frozen? {not unfrozen}')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tutC-model.pt')\n","    \n","    if (epoch + 1) >= FREEZE_FOR:\n","        #unfreeze embeddings\n","        model.embedding.weight.requires_grad = unfrozen = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J2ObMk1PU2jy","colab_type":"code","colab":{}},"source":["if valid_loss < best_valid_loss:\n","    best_valid_loss = valid_loss\n","    torch.save(model.state_dict(), 'tutC-model.pt')\n","else:\n","    #unfreeze embeddings\n","    model.embedding.weight.requires_grad = unfrozen = True"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R-i-lgZIVVna","colab_type":"text"},"source":["# Saving Embeddings"]},{"cell_type":"code","metadata":{"id":"xDVywbPLU75K","colab_type":"code","colab":{}},"source":["from tqdm import tqdm\n","\n","def write_embeddings(path, embeddings, vocab):\n","    \n","    with open(path, 'w') as f:\n","        for i, embedding in enumerate(tqdm(embeddings)):\n","            word = vocab.itos[i]\n","            #skip words with unicode symbols\n","            if len(word) != len(word.encode()):\n","                continue\n","            vector = ' '.join([str(i) for i in embedding.tolist()])\n","            f.write(f'{word} {vector}\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o9sFt8saVaFC","colab_type":"text"},"source":["We'll write our embeddings to trained_embeddings.txt."]},{"cell_type":"code","metadata":{"id":"M_ib00r6VZpq","colab_type":"code","colab":{}},"source":["write_embeddings('custom_embeddings/trained_embeddings.txt', \n","                 model.embedding.weight.data, \n","                 TEXT.vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HwvyYzpJVir6","colab_type":"text"},"source":["To double check they've written correctly, we can load them as Vectors."]},{"cell_type":"code","metadata":{"id":"mtp0txaeVjHS","colab_type":"code","colab":{}},"source":["\n","trained_embeddings = vocab.Vectors(name = 'custom_embeddings/trained_embeddings.txt',\n","                                   cache = 'custom_embeddings',\n","                                   unk_init = torch.Tensor.normal_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8wiNKTiOWxB0","colab_type":"code","colab":{}},"source":["print(trained_embeddings.vectors[:5])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jQB9jTH9Wy7i","colab_type":"code","colab":{}},"source":["print(model.embedding.weight.data[:5])"],"execution_count":0,"outputs":[]}]}